{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codekage/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/codekage/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from glob import glob\n",
    "from re import findall,sub\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import  *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:CPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:XLA_GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:XLA_CPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental_list_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob(\"./lfw_funneled/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = np.array([[\" \".join(i.split(\"/\")[-1].split(\"_\")[:-1]),i] for i in images])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Occurence Of A Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import choice\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "face_occurence = Counter(people[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "face_occurence = pd.DataFrame([{\"name\":i,\"occurence\":face_occurence[i]} for i in face_occurence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>occurence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dita Von Tesse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chris Byrd</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Irfan Ahmed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John Edwards</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Manfred Reyes Villa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  occurence\n",
       "0       Dita Von Tesse          1\n",
       "1           Chris Byrd          2\n",
       "2          Irfan Ahmed          1\n",
       "3         John Edwards          8\n",
       "4  Manfred Reyes Villa          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_occurence.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_use = face_occurence.name[face_occurence.occurence > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haar = cv2.CascadeClassifier()\n",
    "haar.load(\"../haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = []\n",
    "# images = []\n",
    "# failed = []\n",
    "\n",
    "# for name,path in tqdm(people):\n",
    "#     img = cv2.imread(path)\n",
    "#     try:\n",
    "#         gray = img.mean(axis=2).astype(np.uint8)\n",
    "#         x,y,w,h = haar.detectMultiScale(gray)[0]\n",
    "#         img = cv2.resize(img[y:y+h,x:x+w],(128,128))\n",
    "#         names.append(name)\n",
    "#         images.append(img)\n",
    "#     except Exception as e:\n",
    "#         failed.append([name,img,e])\n",
    "    \n",
    "# images = np.array(images)\n",
    "# names = np.array(names)\n",
    "\n",
    "# np.save(\"./names.npy\",names)\n",
    "# np.save(\"./faces.npy\",images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = np.load(\"./names.npy\")\n",
    "images = np.load(\"./faces.npy\").astype(np.float32)\n",
    "images= images / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "for name in set(names_to_use):\n",
    "    group = np.where(name == names)[0]\n",
    "    l = len(group)\n",
    "    for i in range(l):\n",
    "        pairs.append([\n",
    "            group[i],\n",
    "            group[(i+1)%l],\n",
    "            np.random.randint(13190)\n",
    "        ])\n",
    "\n",
    "pairs = np.array(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9460,  9461,  4698],\n",
       "       [ 9461,  9462, 11644],\n",
       "       [ 9462,  9463, 12558],\n",
       "       [ 9463,  9460,  3211],\n",
       "       [ 7404,  7405,  9001],\n",
       "       [ 7405,  7404,  4367],\n",
       "       [10253, 10254, 12505],\n",
       "       [10254, 10253,  2875],\n",
       "       [ 1272,  1273,  6052],\n",
       "       [ 1273,  1274,  1745]])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(64,4,activation=\"relu\",input_shape=(128,128,3)),\n",
    "    keras.layers.Conv2D(64,3,activation=\"relu\"),\n",
    "    keras.layers.Conv2D(64,3,activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    \n",
    "    keras.layers.Conv2D(64,3,activation=\"relu\"),\n",
    "    keras.layers.Conv2D(64,3,activation=\"relu\"),\n",
    "    keras.layers.Conv2D(64,3,activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    \n",
    "    keras.layers.Conv2D(128,3,activation=\"relu\"),\n",
    "    keras.layers.Conv2D(128,3,activation=\"relu\"),\n",
    "    keras.layers.Conv2D(128,3,activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    \n",
    "    keras.layers.Conv2D(128,3,activation=\"relu\"),\n",
    "    keras.layers.Conv2D(128,3,activation=\"relu\"),\n",
    "    keras.layers.Conv2D(128,3,activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "        \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(2048,activation=\"relu\"),\n",
    "    keras.layers.Dense(1024,activation=\"relu\"),\n",
    "    keras.layers.Dense(128,activation=\"linear\"),\n",
    "    keras.layers.BatchNormalization()\n",
    "])\n",
    "\n",
    "anchor = keras.layers.Input(shape=(128,128,3),name=\"anchor\")\n",
    "positive = keras.layers.Input(shape=(128,128,3),name=\"positive\")\n",
    "negative = keras.layers.Input(shape=(128,128,3),name=\"negative\")\n",
    "\n",
    "a_out = model(anchor)\n",
    "p_out = model(positive)\n",
    "n_out = model(negative)\n",
    "\n",
    "out = keras.layers.concatenate([a_out,p_out,n_out],axis=-1)\n",
    "model_train = keras.models.Model([anchor,positive,negative],out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor (InputLayer)             [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive (InputLayer)           [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative (InputLayer)           [(None, 128, 128, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 128)          4280064     anchor[0][0]                     \n",
      "                                                                 positive[0][0]                   \n",
      "                                                                 negative[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 384)          0           sequential_19[1][0]              \n",
      "                                                                 sequential_19[2][0]              \n",
      "                                                                 sequential_19[3][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,280,064\n",
      "Trainable params: 4,279,808\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![triplet_loss](./triplet_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss:    \n",
    "    def __init__(self,margin=0.1):\n",
    "        self.margin = margin\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,y_true,y_pred=None,**kwargs):\n",
    "        a = y_true[:,0::3]\n",
    "        p = y_true[:,1::3]\n",
    "        n = y_true[:,2::3]\n",
    "\n",
    "        pos = keras.backend.sqrt(keras.backend.sum(keras.backend.square(p-a),axis=1))\n",
    "        neg = keras.backend.sqrt(keras.backend.sum(keras.backend.square(n-a),axis=1))\n",
    "        \n",
    "        return tf.maximum(pos - neg,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pairs.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_func(x,batch_size=1,epochs=1):\n",
    "    for epoch in range(epochs):\n",
    "        x = x.reshape(-1,batch_size*3)\n",
    "        for batch in x:\n",
    "            batch = images[batch]\n",
    "            yield {\n",
    "                    \"anchor\":batch[::3],\n",
    "                    \"positive\":batch[1::3],\n",
    "                    \"negative\":batch[2::3]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = TripletLoss()\n",
    "opt = keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n",
      "Epoch : 1 | Loss : 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/api/_v1/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mepoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch : {epoch+1} | Loss : {str(loss_)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    576\u001b[0m   \u001b[0muse_cudnn_on_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m   \u001b[0mshape_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m   \u001b[0;31m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_n\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    375\u001b[0m   \"\"\"\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape_n\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   8632\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   8633\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ShapeN\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8634\u001b[0;31m         name, _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[1;32m   8635\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8636\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 11\n",
    "epochs = 5\n",
    "gen = input_func(train,batch_size=batch_size,epochs=epochs)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_losses = []\n",
    "    for i in range(train.shape[0]//(3*batch_size)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            x = next(gen)\n",
    "            out = model_train(x)\n",
    "            loss = triplet_loss(out)\n",
    "            loss_ = keras.backend.mean(loss).numpy()\n",
    "            epoch_losses.append(loss_)\n",
    "            print (f\"Epoch : {epoch+1} | Loss : {str(loss_)}\")\n",
    "        grad = tape.gradient(loss,model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grad,model.trainable_variables))        \n",
    "        \n",
    "#     print (f\"Epoch : {epoch+1} | Loss : {str(keras.backend.mean(loss).numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
